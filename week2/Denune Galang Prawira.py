# -*- coding: utf-8 -*-
"""Task2_13.09.2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NVioPk-aP4NO2GOxGT66RIMwrrj0rpw

# Task 2. Investigating Relationships (part 1)

Analyze the data on Global AI Job Market & Salary Trends 2025.

[Source](https://www.kaggle.com/datasets/bismasajjad/global-ai-job-market-and-salary-trends-2025)

The file `ai_job_dataset.csv` contains the following variables:

| Variable name | Variable description
|:-------------------:|:------------------:|
| job_id | Unique identifier for each job posting |
| job_title | Standardized job title |
| salary_usd | Annual salary in USD|
| salary_currency | Original salary currency|
| salary_local | Salary in local currency|
| experience_level | EN (Entry), MI (Mid), SE (Senior), EX (Executive)|
| employment_type |	FT (Full-time), PT (Part-time), CT (Contract), FL (Freelance)|
| company_location | Country where company is located|
| company_size | S (Small <50), M (Medium 50-250), L (Large >250)|
| employee_residence |	Country where employee resides|
| remote_ratio | 0 (No remote), 50 (Hybrid), 100 (Fully remote)|
| required_skills |	Top 5 required skills (comma-separated)|
| education_required |	Minimum education requirement|
| years_experience | Required years of experience|
| industry | Industry sector of the company |
| posting_date | Date when job was posted |
| application_deadline | Application deadline |
| job_description_length | Character count of job description |
| benefits_score |	Numerical score of benefits package (1-10) |

Please complete the following tasks.

**1. For interval or ratio scale variables run suitable normality tests to check whether their distribution is close to the normal distribution. Formulate hypothesis. Create graphs to compare the variables’ distributions with the normal distribution. Make conclusions.**
"""

import pandas as pd
from scipy import stats
from scipy.stats import kstest, norm
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import numpy as np
import math
from math import sqrt
import seaborn as sns

df = pd.read_csv('ai_job_dataset.csv')

# Install the pingouin library for partial correlation (if needed)
!pip install pingouin

df

interval_variable = ["salary_usd", "remote_ratio", "years_experience", "job_description_length", "benefits_score"]
df[interval_variable]

from scipy.stats import norm

vars = interval_variable
n = len(vars)
ncols = 3
nrows = math.ceil(n / ncols)

fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*3))
axes = axes.flatten()

for i, var in enumerate(vars):
    ax = axes[i]
    # ax.hist(df[var].dropna(), bins=20)

    sns.distplot(df[var].dropna(), ax=ax, fit=norm)

    ax.set_title(var)
    ax.set_ylabel('Frequency')

for ax in axes[n:]:
    ax.set_visible(False)

plt.tight_layout()
plt.show()

print("USD Salary:")
stats.kstest(df['salary_usd'].dropna(),
             'norm', args=(df['salary_usd'].dropna().mean(),
                           df['salary_usd'].dropna().std()))

print("Remote Ratio:")
stats.kstest(df['remote_ratio'].dropna(),
             'norm', args=(df['remote_ratio'].dropna().mean(),
                           df['remote_ratio'].dropna().std()))

print("year experience:")
stats.kstest(df['years_experience'].dropna(),
             'norm', args=(df['years_experience'].dropna().mean(),
                           df['years_experience'].dropna().std()))

print("job description lenght:")
stats.kstest(df['job_description_length'].dropna(),
             'norm', args=(df['job_description_length'].dropna().mean(),
                           df['job_description_length'].dropna().std()))

print("benefit score:")
stats.kstest(df['benefits_score'].dropna(),
             'norm', args=(df['benefits_score'].dropna().mean(),
                           df['benefits_score'].dropna().std()))

pg.qqplot(df['salary_usd'], dist='norm')
plt.title('SALARY USD')
plt.show()

pg.qqplot(df['remote_ratio'], dist = 'norm')
plt.title('REMOTE RATIO')
plt.show()

pg.qqplot(df['years_experience'], dist = 'norm')
plt.title('YEAR EXPERIENCE')
plt.show()

pg.qqplot(df['job_description_length'], dist = 'norm')
plt.title('JOB DESCRIPTION')
plt.show()

pg.qqplot(df['benefits_score'], dist = 'norm')
plt.title('BENEFIT SCORE')
plt.show()

"""**HYPOTHESIS after comparation**
* `Salary USD`
p-value < 0.05, H0 is rejected, H1 is not rejected => the distribution of salary_usd is significantly different from normal

* `Years of experience`
p-value < 0.05, H0 is rejected, H1 is not rejected => the distribution of years_experience is significantly different from normal

* `Job description length`
p-value < 0.05, H0 is rejected, H1 is not rejected => the distribution of job_description_length is significantly different from normal


* `Remote ratio`
as p-value < 0.05, H0 is rejected, H1 is not rejected => the distribution of remote_ratio is significantly different from normal

* `Benefits score`
p-value < 0.05, H0 is rejected, H1 is not rejected => the distribution of benefits_score is significantly different from normal

**Job description length is found out to be close to normal**

**2. Run a chi-square test to analyze the relationship between categorical variables. You can take the existing variables or create new categorical variables based on interval or ratio scale variables. Explain why the chi-square test is applicable to analyze the relationship between the selected pairs of variables. Create a contingency table based on these variables and describe the tendencies that you can observe in frequency distribution. Formulate hypotheses for the chi-square test, interpret the results of analysis and make conclusions. Create a suitable graph to demonstrate the relationship between the selected variables.**

**Relationship test**

are higher education level categories more frequent for senior or executive roles?

-----------------------------
hypotheses for the chi-square test:
* Null Hypothesis (H0): There is no significant association between {var1} and {var2}.
* Alternative Hypothesis (H1): There is a significant association between {var1} and {var2}.
"""

ct_com_rem = pd.crosstab(df['experience_level'], df['education_required'])
ct_com_rem

stats.chi2_contingency(ct_com_rem)

print('chi2 = ', stats.chi2_contingency(ct_com_rem)[0], '   p-value =', stats.chi2_contingency(ct_com_rem)[1])

df.groupby(['experience_level'])['education_required'].value_counts().unstack().plot(kind='bar', stacked=True)

"""H1: there is a relationship between the experience level and the education required

H0: there is no relationship between the experience level and the education required

p-value > 0.05

p-value > 0.01

H1 is rejected both at 5% and at 1% level of significance, H0 is accepted.

------------

**Conclusion**

P-value: 0.1517 means there is no relations between experience_level and education_required based

**3. Calculate appropriate correlation coefficients between **any three pairs** of variables. Explain the selection of the correlation coefficient. Fill in the table below. Interpret the results. Create suitable graphs to visualize the analyzed relationships.**
"""

variable_pairs = [
    ('years_experience', 'benefits_score'),
    ('salary_usd', 'years_experience'),
    ('salary_usd', 'benefits_score')
]


correlation_results = []


for var1, var2 in variable_pairs:
    print(f"Analyzing relationship between {var1} and {var2} using Pearson correlation:")


    data = df[[var1, var2]].dropna()

    if len(data) < 2:
        print(f"  Not enough data points ({len(data)}) to calculate correlation for {var1} and {var2}.")
        correlation_results.append({
            'Variables': f'{var1} & {var2}',
            'Appropriate correlation coefficient(justification of the choice)': 'Pearson (Interval/Ratio Scale)',
            'Hypotheses': f'H0: ρ=0, H1: ρ≠0',
            'Strength of the relationship': 'N/A (Insufficient data)',
            'Direction of the relationship': 'N/A (Insufficient data)',
            'Statistical significance of the relationship!': 'N/A (Insufficient data)'
        })
        print("-" * 50)
        continue


    correlation_coefficient, p_value = pearsonr(data[var1], data[var2])


    abs_corr = abs(correlation_coefficient)
    if abs_corr < 0.3:
        strength = 'Weak'
    elif abs_corr < 0.7:
        strength = 'Moderate'
    else:
        strength = 'Strong'


    if correlation_coefficient > 0:
        direction = 'Positive'
    elif correlation_coefficient < 0:
        direction = 'Negative'
    else:
        direction = 'No clear direction'


    alpha = 0.05
    if p_value < alpha:
        significance = f'Significant (p={p_value:.4f} < {alpha})'
    else:
        significance = f'Not Significant (p={p_value:.4f} >= {alpha})'

    print(f"  Pearson Correlation Coefficient: {correlation_coefficient:.4f}")
    print(f"  P-value: {p_value:.4f}")
    print(f"  Strength: {strength}")
    print(f"  Direction: {direction}")
    print(f"  Statistical Significance: {significance}")



    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=data, x=var1, y=var2, alpha=0.6)
    plt.title(f'Scatter Plot of {var1} vs {var2}\nPearson r = {correlation_coefficient:.4f}')
    plt.xlabel(var1)
    plt.ylabel(var2)
    plt.grid(True)
    plt.show()


    correlation_results.append({
        'Variables': f'{var1} & {var2}',
        'Appropriate correlation coefficient(justification of the choice)': 'Pearson (Both are continuous interval/ratio scale variables suitable for assessing linear relationship)',
        'Hypotheses': 'H0: ρ=0, H1: ρ≠0',
        'Strength of the relationship': strength,
        'Direction of the relationship': direction,
        'Statistical significance of the relationship!': significance
    })

    print("-" * 50)

print("\nCorrelation Analysis Results Table:")
correlation_table_df = pd.DataFrame(correlation_results)
display(correlation_table_df)

"""| Variables | Appropriate correlation coefficient(justification of the choice) | Hypotheses | Strength of the relationship | Direction of the relationship | Statistical significance of the relationship! |
| --- | --- | --- | --- | --- | --- |
| years_experience and benefits_score  | Pearson - Both are continuous interval/ratio | H0 | weak (r= -0.0073) | negative | Not significant (p=0.3730 >= 0.05) |
| salary_usd and years_experience | Pearson - Both are continuous interval/ratio  | H1 | strong (r=0.7376) | positive | Significant (p=0.0000 < 0.05) |
| salary_usd and benefits_score | Pearson - Both are continuous interval/ratio | H0 | weak (r= 0.0010)| negative | Not Significant (p=0.9040 >= 0.05) |

**4. Calculate a paired correlation coefficient between any variables. Then calculate the partial correlation coefficient between the same pair of variables controlling for any other third variable. Interpret the results of analysis. Create suitable graphs to visualize the analyzed relationships.**
"""

X1 = sm.add_constant(subset['benefits_score'])
model1 = sm.OLS(subset['salary_usd'], X1).fit()
resid_salary = model1.resid

model2 = sm.OLS(subset['years_experience'], X1).fit()
resid_experience = model2.resid

partial_corr_value, partial_pval = pearsonr(resid_salary, resid_experience)

(corr, p_val, partial_corr_value, partial_pval)

df1 = df.dropna(subset=['salary_usd', 'years_experience'])
stats.spearmanr(df1['salary_usd'], df1['years_experience'])

df = df.dropna(subset=['salary_usd', 'years_experience', 'benefits_score'])
a_bc = stats.spearmanr(df['salary_usd'], df['years_experience'])[0]
a_bd = stats.spearmanr(df['salary_usd'], df['benefits_score'])[0]
a_cd = stats.spearmanr(df['years_experience'], df['benefits_score'])[0]
a_bc_d = (a_bc - a_bd * a_cd) / sqrt((1 - a_bd ** 2) * (1 - a_cd ** 2))
print('Paired correlation between salary_usd and year_experience: ',a_bc)
print('Partial correlation between salary_usd and year_experience, controlling for benefits_score: ',a_bc_d)

pg.partial_corr(data=df, x='salary_usd',
                y='years_experience', covar='job_description_length', method = 'spearman')

subset = df.dropna(subset=['salary_usd', 'years_experience', 'benefits_score'])
X1 = sm.add_constant(subset['benefits_score'])

model1 = sm.OLS(subset['salary_usd'], X1).fit()
resid_salary = model1.resid

model2 = sm.OLS(subset['years_experience'], X1).fit()
resid_experience = model2.resid

plt.figure(figsize=(8, 6))
sns.scatterplot(x=resid_experience, y=resid_salary, alpha=0.5)
sns.regplot(x=resid_experience, y=resid_salary, scatter=False, color='red')
plt.title('Partial Relationship: Salary USD vs Years Experience\ncontrolling for Benefits Score')
plt.xlabel('Years of Experience (partial residuals)')
plt.ylabel('Salary USD (partial residuals)')
plt.grid(True)
plt.show()

subset2 = df.dropna(subset=['salary_usd', 'years_experience', 'benefits_score'])
X2 = sm.add_constant(subset2['salary_usd'])

model3 = sm.OLS(subset2['years_experience'], X2).fit()
resid_experience2 = model3.resid

model4 = sm.OLS(subset2['benefits_score'], X2).fit()
resid_benefits = model4.resid

plt.figure(figsize=(8, 6))
sns.scatterplot(x=resid_benefits, y=resid_experience2, alpha=0.5)
sns.regplot(x=resid_benefits, y=resid_experience2, scatter=False, color='red')
plt.title('Partial Relationship: Years Experience vs Benefits Score\ncontrolling for Salary USD')
plt.xlabel('Benefits Score (partial residuals)')
plt.ylabel('Years of Experience (partial residuals)')
plt.grid(True)
plt.show()

subset3 = df.dropna(subset=['salary_usd', 'years_experience', 'benefits_score'])
X3 = sm.add_constant(subset3['years_experience'])

model5 = sm.OLS(subset3['salary_usd'], X3).fit()
resid_salary3 = model5.resid

model6 = sm.OLS(subset3['benefits_score'], X3).fit()
resid_benefits3 = model6.resid

plt.figure(figsize=(8, 6))
sns.scatterplot(x=resid_benefits3, y=resid_salary3, alpha=0.5)
sns.regplot(x=resid_benefits3, y=resid_salary3, scatter=False, color='red')
plt.title('Partial Relationship: Salary USD vs Benefits Score\ncontrolling for Years Experience')
plt.xlabel('Benefits Score (partial residuals)')
plt.ylabel('Salary USD (partial residuals)')
plt.grid(True)
plt.show()

"""r = 0.737583 = positive relationship between the two variables (salary and years of experience).

CI95% = [0.79, 0.8] = CI95% confident that the true correlation lies between 0.79 and 0.8

p-val = 0.0 = correlation is significant.
"""