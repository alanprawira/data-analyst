# -*- coding: utf-8 -*-
"""Task2_13.09.2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NVioPk-aP4NO2GOxGT66RIMwrrj0rpw

# Task 2. Investigating Relationships (part 1)

Analyze the data on Global AI Job Market & Salary Trends 2025.

[Source](https://www.kaggle.com/datasets/bismasajjad/global-ai-job-market-and-salary-trends-2025)

The file `ai_job_dataset.csv` contains the following variables:

| Variable name | Variable description
|:-------------------:|:------------------:|
| job_id | Unique identifier for each job posting |
| job_title | Standardized job title |
| salary_usd | Annual salary in USD|
| salary_currency | Original salary currency|
| salary_local | Salary in local currency|
| experience_level | EN (Entry), MI (Mid), SE (Senior), EX (Executive)|
| employment_type |	FT (Full-time), PT (Part-time), CT (Contract), FL (Freelance)|
| company_location | Country where company is located|
| company_size | S (Small <50), M (Medium 50-250), L (Large >250)|
| employee_residence |	Country where employee resides|
| remote_ratio | 0 (No remote), 50 (Hybrid), 100 (Fully remote)|
| required_skills |	Top 5 required skills (comma-separated)|
| education_required |	Minimum education requirement|
| years_experience | Required years of experience|
| industry | Industry sector of the company |
| posting_date | Date when job was posted |
| application_deadline | Application deadline |
| job_description_length | Character count of job description |
| benefits_score |	Numerical score of benefits package (1-10) |

Please complete the following tasks.

**1. For interval or ratio scale variables run suitable normality tests to check whether their distribution is close to the normal distribution. Formulate hypothesis. Create graphs to compare the variables’ distributions with the normal distribution. Make conclusions.**
"""

df = pd.read_csv('ai_job_dataset.csv')

df

print(df.columns)

"""Based on the variable descriptions and common statistical definitions:

*   **`salary_usd`**: This is a **ratio** variable. A salary of $0 indicates the complete absence of a salary.
*   **`years_experience`**: This is a **ratio** variable. 0 years means no experience.
*   **`job_description_length`**: This is a **ratio** variable.
*   **`remote_ratio`**: This represents a percentage (0, 50, 100) and can be treated as a **ratio** variable as 0 means no remote work
*   **`benefits_score`**: This is likely an **interval** variable. The scale (1-10) has equal intervals

"""

import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import numpy as np

interval_ratio_vars = ['salary_usd', 'years_experience', 'job_description_length', 'remote_ratio', 'benefits_score']
alpha = 0.05

for var in interval_ratio_vars:
    print(f"Analyzing variable: {var}")
    data = df[var].dropna()

    if len(data) < 3:
        print(f"  Not enough data points ({len(data)}) to perform normality tests for {var}.")
        print("-" * 50)
        continue

    try:
        ks_statistic, ks_pvalue = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
        print(f"  Kolmogorov-Smirnov Test:")
        print(f"    Statistic: {ks_statistic:.4f}")
        print(f"    P-value: {ks_pvalue:.4f}")
    except Exception as e:
         print(f"  Could not perform Kolmogorov-Smirnov test for {var}: {e}")
         ks_pvalue = None


    print("  Interpretation of Test Results:")
    if ks_pvalue is not None:
        if ks_pvalue < alpha:
             print(f"    With a p-value ({ks_pvalue:.4f}) less than alpha ({alpha}), we reject the null hypothesis.")
             print("    Conclusion: The distribution is significantly different from a normal distribution based on the Kolmogorov-Smirnov test.")
        else:
             print(f"    With a p-value ({ks_pvalue:.4f}) greater than or equal to alpha ({alpha}), we fail to reject the null hypothesis.")
             print("    Conclusion: The distribution is not significantly different from a normal distribution based on the Kolmogorov-Smirnov test.")
    else:
        print("    Kolmogorov-Smirnov test was not performed or failed.")


    plt.figure(figsize=(10, 6))
    sns.histplot(data, kde=True, bins=50)
    plt.title(f'Distribution of {var} with KDE')
    plt.xlabel(var)
    plt.ylabel('Frequency')
    plt.show()

    # Q-Q Plot
    plt.figure(figsize=(8, 8))
    sm.qqplot(data, line='s')
    plt.title(f'Q-Q Plot of {var}')
    plt.show()

    print("-" * 50) # Separator for clarity between variables

"""**HYPOTHESIS after comparation**
* Salary (salary_usd)
Histogram: right skewed (long tail).
Q–Q plot: deviation
KS statistic = 0.1063
Not normal

* Years of experience
Histogram: skewed.
Q–Q plot: deviation from diagonal.
KS statistic = 0.1577.
Not normal.

* Job description length
Histogram: somewhat bell-shaped but still skewed.
Q–Q plot: points follow the diagonal, but deviate at tails.
KS statistic = 0.0585
Close to normal

* Remote ratio
Histogram: only three discrete values (0, 50, 100).
Q–Q plot: difficult to analyze
KS statistic = 0.2257 bad deviation.
not normal (look more like categorical than not ratio).

* Benefits score
Histogram: bounded between 1–10, discrete-like distribution.
Q–Q plot: deviates
KS statistic = 0.0697
Not normal

**Job description length is found out to be close to normal**

**2. Run a chi-square test to analyze the relationship between categorical variables. You can take the existing variables or create new categorical variables based on interval or ratio scale variables. Explain why the chi-square test is applicable to analyze the relationship between the selected pairs of variables. Create a contingency table based on these variables and describe the tendencies that you can observe in frequency distribution. Formulate hypotheses for the chi-square test, interpret the results of analysis and make conclusions. Create a suitable graph to demonstrate the relationship between the selected variables.**
"""



"""**Relationship test**

are higher education level categories more frequent for senior or executive roles?

-----------------------------
hypotheses for the chi-square test:
* Null Hypothesis (H0): There is no significant association between {var1} and {var2}.
* Alternative Hypothesis (H1): There is a significant association between {var1} and {var2}.

"""

from scipy.stats import chi2_contingency

var1 = 'experience_level'
var2 = 'education_required'

contingency_table = pd.crosstab(df[var1], df[var2])
print(f"Contingency Table of {var1} vs {var2}:")
display(contingency_table)

# Chi-Square Test
chi2_statistic, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)

print(f"\nChi-Square Test Results for {var1} vs {var2}:")
print(f"  Chi-Square Statistic: {chi2_statistic:.4f}")
print(f"  P-value: {p_value:.4f}")
print(f"  Degrees of Freedom: {dof}")

alpha = 0.05
print("\nInterpretation and Conclusion:")
if p_value < alpha:
    print(f"  With a p-value ({p_value:.4f}) less than alpha ({alpha}), we reject the null hypothesis.")
    print(f"  Conclusion: There is a statistically significant association between {var1} and {var2}.")
    print("  This suggests that the distribution of required education levels is dependent on the experience level.")
else:
    print(f"  With a p-value ({p_value:.4f}) greater than or equal to alpha ({alpha}), we fail to reject the null hypothesis.")
    print(f"  Conclusion: There is no statistically significant association between {var1} and {var2}.")
    print("  This suggests that the distribution of required education levels is independent of the experience level.")

plt.figure(figsize=(10, 7))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')
plt.title(f'Relationship between {var1} and {var2}')
plt.xlabel(var2)
plt.ylabel(var1)
plt.show()

contingency_table.plot(kind='bar', stacked=True, figsize=(10, 7))
plt.title(f'Relationship between {var1} and {var2} (Stacked Bar Chart)')
plt.xlabel(var1)
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.legend(title=var2)
plt.show()

"""The chi-square test for independence is applicable here because we are examining the relationship between two categorical variables: 'experience_level' and 'salary_category'.
We want to determine if there is a statistically significant association between. the level of experience and the salary category. The test compares the observed frequencies in a contingency table to the frequencies that would be expected if the two variables were independent.

**Conclusion**

P-value: 0.1517 means there is no statistically significant association between experience_level and education_required based on this test.

**3. Calculate appropriate correlation coefficients between **any three pairs** of variables. Explain the selection of the correlation coefficient. Fill in the table below. Interpret the results. Create suitable graphs to visualize the analyzed relationships.**
"""

# your code here



from scipy.stats import pearsonr
import matplotlib.pyplot as plt


variable_pairs = [
    ('years_experience', 'benefits_score'),
    ('salary_usd', 'years_experience'),
    ('salary_usd', 'benefits_score')
]


correlation_results = []


for var1, var2 in variable_pairs:
    print(f"Analyzing relationship between {var1} and {var2} using Pearson correlation:")


    data = df[[var1, var2]].dropna()

    if len(data) < 2:
        print(f"  Not enough data points ({len(data)}) to calculate correlation for {var1} and {var2}.")
        correlation_results.append({
            'Variables': f'{var1} & {var2}',
            'Appropriate correlation coefficient(justification of the choice)': 'Pearson (Interval/Ratio Scale)',
            'Hypotheses': f'H0: ρ=0, H1: ρ≠0',
            'Strength of the relationship': 'N/A (Insufficient data)',
            'Direction of the relationship': 'N/A (Insufficient data)',
            'Statistical significance of the relationship!': 'N/A (Insufficient data)'
        })
        print("-" * 50)
        continue


    correlation_coefficient, p_value = pearsonr(data[var1], data[var2])


    abs_corr = abs(correlation_coefficient)
    if abs_corr < 0.3:
        strength = 'Weak'
    elif abs_corr < 0.7:
        strength = 'Moderate'
    else:
        strength = 'Strong'


    if correlation_coefficient > 0:
        direction = 'Positive'
    elif correlation_coefficient < 0:
        direction = 'Negative'
    else:
        direction = 'No clear direction'


    alpha = 0.05
    if p_value < alpha:
        significance = f'Significant (p={p_value:.4f} < {alpha})'
    else:
        significance = f'Not Significant (p={p_value:.4f} >= {alpha})'

    print(f"  Pearson Correlation Coefficient: {correlation_coefficient:.4f}")
    print(f"  P-value: {p_value:.4f}")
    print(f"  Strength: {strength}")
    print(f"  Direction: {direction}")
    print(f"  Statistical Significance: {significance}")



    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=data, x=var1, y=var2, alpha=0.6)
    plt.title(f'Scatter Plot of {var1} vs {var2}\nPearson r = {correlation_coefficient:.4f}')
    plt.xlabel(var1)
    plt.ylabel(var2)
    plt.grid(True)
    plt.show()


    correlation_results.append({
        'Variables': f'{var1} & {var2}',
        'Appropriate correlation coefficient(justification of the choice)': 'Pearson (Both are continuous interval/ratio scale variables suitable for assessing linear relationship)',
        'Hypotheses': 'H0: ρ=0, H1: ρ≠0',
        'Strength of the relationship': strength,
        'Direction of the relationship': direction,
        'Statistical significance of the relationship!': significance
    })

    print("-" * 50)

print("\nCorrelation Analysis Results Table:")
correlation_table_df = pd.DataFrame(correlation_results)
display(correlation_table_df)

"""| Variables | Appropriate correlation coefficient(justification of the choice) | Hypotheses | Strength of the relationship | Direction of the relationship | Statistical significance of the relationship! |
| --- | --- | --- | --- | --- | --- |
| years_experience and benefits_score  | Pearson - Both are continuous interval/ratio | H0 | weak | negative | Not significant (p=0.3730 >= 0.05) |
| salary_usd and years_experience | Pearson - Both are continuous interval/ratio  | H1 | strong | positive | Significant (p=0.0000 < 0.05) |
| salary_usd and benefits_score | Pearson - Both are continuous interval/ratio | H0 | weak | negative | Not Significant (p=0.9040 >= 0.05) |

**4. Calculate a paired correlation coefficient between any variables. Then calculate the partial correlation coefficient between the same pair of variables controlling for any other third variable. Interpret the results of analysis. Create suitable graphs to visualize the analyzed relationships.**
"""

# Install the pingouin library for partial correlation (if needed)
!pip install pingouin

import pingouin as pg

df['salary_usd'] = pd.to_numeric(df['salary_usd'], errors='coerce')

subset = df[['salary_usd', 'years_experience', 'benefits_score']].dropna()

corr, p_val = pearsonr(subset['salary_usd'], subset['years_experience'])

partial_corr = pg.partial_corr(data=subset,
                               x='salary_usd',
                               y='years_experience',
                               covar='benefits_score',
                               method='pearson')

plt.scatter(subset['years_experience'], subset['salary_usd'], alpha=0.6)
m, b = np.polyfit(subset['years_experience'], subset['salary_usd'], 1)
plt.plot(subset['years_experience'], m*subset['years_experience'] + b, color='red')
plt.xlabel("Years of Experience")
plt.ylabel("Salary (USD)")
plt.title("Salary vs Years of Experience")
plt.show()

(corr, p_val, partial_corr)

X1 = sm.add_constant(subset['benefits_score'])
model1 = sm.OLS(subset['salary_usd'], X1).fit()
resid_salary = model1.resid

model2 = sm.OLS(subset['years_experience'], X1).fit()
resid_experience = model2.resid

partial_corr_value, partial_pval = pearsonr(resid_salary, resid_experience)

(corr, p_val, partial_corr_value, partial_pval)

"""* r = 0.737583 = The correlation coefficient.

This means there’s a strong positive relationship between the two variables (salary and years of experience).

* CI95% = [0.73, 0.74] → The 95% confidence interval for the correlation.

CI95% confident that the true correlation lies between 0.73 and 0.74. estimate is very precise due to the large sample size.

* p-val = 0.0 = The p-value is effectively zero.

This means the correlation is highly statistically significant.

Salary and years of experience are strongly and positively correlated.
The relationship is statistically significant (p < 0.001).

Confidence intervals show that the correlation consistently falls between 0.73 and 0.74, which confirms reliability.
"""